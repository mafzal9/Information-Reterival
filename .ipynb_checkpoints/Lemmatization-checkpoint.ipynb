{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n",
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "#\n",
    "#       Subject= Information Reterival\n",
    "#\n",
    "#       Author= Muhammad Afzal\n",
    "#\n",
    "#       Registration# MSITS07203002\n",
    "#\n",
    "#       Example=Lemmitzation\n",
    "#\n",
    "###########################################################\n",
    "\n",
    "#Wordnet Lemmatizer with NLTK\n",
    "import nltk\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "\n",
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('feet', 'NNS')]\n",
      "[('The', 'DT'), ('striped', 'JJ'), ('bats', 'NNS'), ('are', 'VBP'), ('hanging', 'VBG'), ('on', 'IN'), ('their', 'PRP$'), ('feet', 'NNS'), ('for', 'IN'), ('best', 'JJS')]\n",
      "foot\n",
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "#Wordnet Lemmatizer with appropriate POS tag\n",
    "print(nltk.pos_tag(['feet']))\n",
    "\n",
    "print(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the stripe bat be hang on their foot for good'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spaCy Lemmatization\n",
    "import spacy\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Parse the sentence using the loaded 'en' model object `nlp`\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the lemma for each token and join\n",
    "\" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat are hanging on their foot for best'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TextBlob Lemmatizer\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "# Lemmatize a word\n",
    "word = 'stripes'\n",
    "w = Word(word)\n",
    "w.lemmatize()\n",
    "# Lemmatize a sentence\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "sent = TextBlob(sentence)\n",
    "\" \". join([w.lemmatize() for w in sent.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The striped bat be hang on their foot for best'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TextBlob Lemmatizer with appropriate POS tag\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "# Lemmatize\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "lemmatize_with_postag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The/DT/the striped/JJ/striped bats/NNS/bat were/VBD/be hanging/VBG/hang on/IN/on their/PRP$/their feet/NNS/foot and/CC/and ate/VBD/eat best/JJ/best fishes/NNS/fish\n"
     ]
    }
   ],
   "source": [
    "#Pattern Lemmatizer\n",
    "import pattern\n",
    "from pattern.en import lemma, lexeme\n",
    "\n",
    "sentence = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "\" \".join([lemma(wd) for wd in sentence.split()])\n",
    "# Lexeme's for each word \n",
    "[lexeme(wd) for wd in sentence.split()]\n",
    "\n",
    "from pattern.en import parse\n",
    "print(parse('The striped bats were hanging on their feet and ate best fishes', \n",
    "            lemmata=True, tags=False, chunks=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentences': [{'index': 0,\n",
       "   'tokens': [{'index': 1,\n",
       "     'word': 'The',\n",
       "     'originalText': 'The',\n",
       "     'lemma': 'the',\n",
       "     'characterOffsetBegin': 0,\n",
       "     'characterOffsetEnd': 3,\n",
       "     'pos': 'DT',\n",
       "     'before': '',\n",
       "     'after': ' '},\n",
       "    {'index': 2,\n",
       "     'word': 'striped',\n",
       "     'originalText': 'striped',\n",
       "     'lemma': 'striped',\n",
       "     'characterOffsetBegin': 4,\n",
       "     'characterOffsetEnd': 11,\n",
       "     'pos': 'JJ',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 3,\n",
       "     'word': 'bats',\n",
       "     'originalText': 'bats',\n",
       "     'lemma': 'bat',\n",
       "     'characterOffsetBegin': 12,\n",
       "     'characterOffsetEnd': 16,\n",
       "     'pos': 'NNS',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 4,\n",
       "     'word': 'were',\n",
       "     'originalText': 'were',\n",
       "     'lemma': 'be',\n",
       "     'characterOffsetBegin': 17,\n",
       "     'characterOffsetEnd': 21,\n",
       "     'pos': 'VBD',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 5,\n",
       "     'word': 'hanging',\n",
       "     'originalText': 'hanging',\n",
       "     'lemma': 'hang',\n",
       "     'characterOffsetBegin': 22,\n",
       "     'characterOffsetEnd': 29,\n",
       "     'pos': 'VBG',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 6,\n",
       "     'word': 'on',\n",
       "     'originalText': 'on',\n",
       "     'lemma': 'on',\n",
       "     'characterOffsetBegin': 30,\n",
       "     'characterOffsetEnd': 32,\n",
       "     'pos': 'IN',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 7,\n",
       "     'word': 'their',\n",
       "     'originalText': 'their',\n",
       "     'lemma': 'they',\n",
       "     'characterOffsetBegin': 33,\n",
       "     'characterOffsetEnd': 38,\n",
       "     'pos': 'PRP$',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 8,\n",
       "     'word': 'feet',\n",
       "     'originalText': 'feet',\n",
       "     'lemma': 'foot',\n",
       "     'characterOffsetBegin': 39,\n",
       "     'characterOffsetEnd': 43,\n",
       "     'pos': 'NNS',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 9,\n",
       "     'word': 'and',\n",
       "     'originalText': 'and',\n",
       "     'lemma': 'and',\n",
       "     'characterOffsetBegin': 44,\n",
       "     'characterOffsetEnd': 47,\n",
       "     'pos': 'CC',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 10,\n",
       "     'word': 'ate',\n",
       "     'originalText': 'ate',\n",
       "     'lemma': 'eat',\n",
       "     'characterOffsetBegin': 48,\n",
       "     'characterOffsetEnd': 51,\n",
       "     'pos': 'VBD',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 11,\n",
       "     'word': 'best',\n",
       "     'originalText': 'best',\n",
       "     'lemma': 'best',\n",
       "     'characterOffsetBegin': 52,\n",
       "     'characterOffsetEnd': 56,\n",
       "     'pos': 'JJS',\n",
       "     'before': ' ',\n",
       "     'after': ' '},\n",
       "    {'index': 12,\n",
       "     'word': 'fishes',\n",
       "     'originalText': 'fishes',\n",
       "     'lemma': 'fish',\n",
       "     'characterOffsetBegin': 57,\n",
       "     'characterOffsetEnd': 63,\n",
       "     'pos': 'NNS',\n",
       "     'before': ' ',\n",
       "     'after': ''}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stanford CoreNLP Lemmatization\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "# Connect to the CoreNLP server we just started\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000, timeout=30000)\n",
    "\n",
    "# Define proporties needed to get lemma\n",
    "props = {'annotators': 'pos,lemma',\n",
    "         'pipelineLanguage': 'en',\n",
    "         'outputFormat': 'json'}\n",
    "\n",
    "\n",
    "sentence = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "parsed_str = nlp.annotate(sentence, properties=props)\n",
    "parsed_dict = json.loads(parsed_str)\n",
    "parsed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the striped bat be hang on they foot and eat best fish'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_list = [v for d in parsed_dict['sentences'][0]['tokens'] for k,v in d.items() if k == 'lemma']\n",
    "\" \".join(lemma_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the striped bat be hang on they foot and eat best fish'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json, string\n",
    "\n",
    "def lemmatize_corenlp(conn_nlp, sentence):\n",
    "    props = {\n",
    "        'annotators': 'pos,lemma',\n",
    "        'pipelineLanguage': 'en',\n",
    "        'outputFormat': 'json'\n",
    "    }\n",
    "\n",
    "    # tokenize into words\n",
    "    sents = conn_nlp.word_tokenize(sentence)\n",
    "\n",
    "    # remove punctuations from tokenised list\n",
    "    sents_no_punct = [s for s in sents if s not in string.punctuation]\n",
    "\n",
    "    # form sentence\n",
    "    sentence2 = \" \".join(sents_no_punct)\n",
    "\n",
    "    # annotate to get lemma\n",
    "    parsed_str = conn_nlp.annotate(sentence2, properties=props)\n",
    "    parsed_dict = json.loads(parsed_str)\n",
    "\n",
    "    # extract the lemma for each word\n",
    "    lemma_list = [v for d in parsed_dict['sentences'][0]['tokens'] for k,v in d.items() if k == 'lemma']\n",
    "\n",
    "    # form sentence and return it\n",
    "    return \" \".join(lemma_list)\n",
    "\n",
    "\n",
    "# make the connection and call `lemmatize_corenlp`\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000, timeout=30000)\n",
    "lemmatize_corenlp(conn_nlp=nlp, sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['striped', 'bat', 'be', 'hang', 'foot', 'eat', 'best', 'fish']\n"
     ]
    }
   ],
   "source": [
    "#Gensim Lemmatize\n",
    "from gensim.utils import lemmatize\n",
    "sentence = \"The striped bats were hanging on their feet and ate best fishes\"\n",
    "lemmatized_out = [wd.decode('utf-8').split('/')[0] for wd in lemmatize(sentence)]\n",
    "print(lemmatized_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'and', 'eat', 'good', 'fish']\n"
     ]
    }
   ],
   "source": [
    "#TreeTagger\n",
    "import treetaggerwrapper as ttpw\n",
    "tagger = ttpw.TreeTagger(TAGLANG='en')\n",
    "tags = tagger.tag_text(\"The striped bats were hanging on their feet and ate best fishes\")\n",
    "lemmas = [t.split('\\t')[-1] for t in tags]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Following mouse attack care farmer be march to Delhi for well living '\n",
      " 'condition Delhi police on Tuesday fire water cannon and teargas shell at '\n",
      " 'protest farmer a they try to break barricade with their car automobile and '\n",
      " 'tractor')\n",
      "('follow mouse attack , care farmer be march to Delhi for well living '\n",
      " 'condition . \\n'\n",
      " ' Delhi police on Tuesday fire water cannon and teargas shell at protest '\n",
      " 'farmer as they try to \\n'\n",
      " ' break barricade with their car , automobile and tractor .')\n",
      "('Following mouse attack care farmer be march to Delhi for good living '\n",
      " 'condition Delhi police on Tuesday fire water cannon and teargas shell at '\n",
      " 'protest farmer a they try to break barricade with their car automobile and '\n",
      " 'tractor')\n",
      "('follow mice attacks, care farmer be march to delhi for better live '\n",
      " 'conditions. delhi police on tuesday fire water cannon and tearga shell at '\n",
      " 'protest farmer a they try to break barricade with their cars, automobile and '\n",
      " 'tractors.')\n",
      "('follow mouse attack care farmer be march to Delhi for better living '\n",
      " 'condition Delhi police on Tuesday fire water cannon and tearga shell at '\n",
      " 'protest farmer as they try to break barricade with they car automobile and '\n",
      " 'tractor')\n"
     ]
    }
   ],
   "source": [
    "#Comparing NLTK, TextBlob, spaCy, Pattern and Stanford CoreNLP\n",
    "sentence = \"\"\"Following mice attacks, caring farmers were marching to Delhi for better living conditions. \n",
    "Delhi police on Tuesday fired water cannons and teargas shells at protesting farmers as they tried to \n",
    "break barricades with their cars, automobiles and tractors.\"\"\"\n",
    "\n",
    "# NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "from pprint import pprint\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json, string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "pprint(\" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence) if w not in string.punctuation]))\n",
    "# ('Following mouse attack care farmer be march to Delhi for well living '\n",
    "#  'condition Delhi police on Tuesday fire water cannon and teargas shell at '\n",
    "#  'protest farmer a they try to break barricade with their car automobile and '\n",
    "#  'tractor')\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "pprint(\" \".join([token.lemma_ for token in doc]))\n",
    "# ('follow mice attack , care farmer be march to delhi for good living condition '\n",
    "#  '. delhi police on tuesday fire water cannon and teargas shell at protest '\n",
    "#  'farmer as -PRON- try to break barricade with -PRON- car , automobile and '\n",
    "#  'tractor .')\n",
    "\n",
    "\n",
    "# TextBlob\n",
    "def lemmatize_with_postag(sentence):\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "pprint(lemmatize_with_postag(sentence))\n",
    "# ('Following mouse attack care farmer be march to Delhi for good living '\n",
    "#  'condition Delhi police on Tuesday fire water cannon and teargas shell at '\n",
    "#  'protest farmer a they try to break barricade with their car automobile and '\n",
    "#  'tractor')\n",
    "\n",
    "# Pattern\n",
    "from pattern.en import lemma\n",
    "pprint(\" \".join([lemma(wd) for wd in sentence.split()]))\n",
    "# ('follow mice attacks, care farmer be march to delhi for better live '\n",
    "#  'conditions. delhi police on tuesday fire water cannon and tearga shell at '\n",
    "#  'protest farmer a they try to break barricade with their cars, automobile and '\n",
    "#  'tractors.')\n",
    "\n",
    "# Stanford\n",
    "def lemmatize_corenlp(conn_nlp, sentence):\n",
    "    props = {\n",
    "        'annotators': 'pos,lemma',\n",
    "        'pipelineLanguage': 'en',\n",
    "        'outputFormat': 'json'\n",
    "    }\n",
    "\n",
    "    # tokenize into words\n",
    "    sents = conn_nlp.word_tokenize(sentence)\n",
    "\n",
    "    # remove punctuations from tokenised list\n",
    "    sents_no_punct = [s for s in sents if s not in string.punctuation]\n",
    "\n",
    "    # form sentence\n",
    "    sentence2 = \" \".join(sents_no_punct)\n",
    "\n",
    "    # annotate to get lemma\n",
    "    parsed_str = conn_nlp.annotate(sentence2, properties=props)\n",
    "    parsed_dict = json.loads(parsed_str)\n",
    "\n",
    "    # extract the lemma for each word\n",
    "    lemma_list = [v for d in parsed_dict['sentences'][0]['tokens'] for k,v in d.items() if k == 'lemma']\n",
    "\n",
    "    # form sentence and return it\n",
    "    return \" \".join(lemma_list)\n",
    "\n",
    "\n",
    "# make the connection and call `lemmatize_corenlp`\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000, timeout=30000)\n",
    "pprint(lemmatize_corenlp(conn_nlp=nlp, sentence=sentence))\n",
    "# ('follow mouse attack care farmer be march to Delhi for better living '\n",
    "#  'condition Delhi police on Tuesday fire water cannon and tearga shell at '\n",
    "#  'protest farmer as they try to break barricade with they car automobile and '\n",
    "#  'tractor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
